{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VK2NXbx9CpXq"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "AMorrJY8DMZ7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain_chroma"
      ],
      "metadata": {
        "id": "EUsQY6SeDieZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "3FRtAKTmDlSN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUoAgUWZCpX4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from  langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings.sentence_transformer import (\n",
        "    SentenceTransformerEmbeddings,\n",
        ")\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "import pandas as pd\n",
        "from langchain.agents import create_sql_agent\n",
        "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
        "from langchain.sql_database import SQLDatabase\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGeufI6XCpX9"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "sampled_df = pd.read_csv(\"./sampled_data.csv\")\n",
        "\n",
        "\n",
        "\n",
        "# Select only the 'question' and 'sql_query' columns\n",
        "df_selected = sampled_df[['question', 'sql_query']]\n",
        "\n",
        "# Save the new DataFrame to a new CSV file\n",
        "df_selected.to_csv('data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkkbyCmlCpX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62292ab2-9765-4266-9c15-fe33053e05b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Names:\n",
            "Index(['question', 'sql_query'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "sampled_df = pd.read_csv(\"./data.csv\")\n",
        "# Split the data into training (70%), validation (15%), and test (15%) sets\n",
        "train_df, temp_df = train_test_split(sampled_df, test_size=0.3, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Show column names and sample data\n",
        "print(\"Column Names:\")\n",
        "print(sampled_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFoovZ8dCpYC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the SQL queries and questions\n",
        "sql_tokenizer = Tokenizer()\n",
        "sql_tokenizer.fit_on_texts(train_df['sql_query'])\n",
        "\n",
        "question_tokenizer = Tokenizer()\n",
        "question_tokenizer.fit_on_texts(train_df['question'])\n",
        "\n",
        "# Convert text to sequences\n",
        "train_sql_seq = sql_tokenizer.texts_to_sequences(train_df['sql_query'])\n",
        "train_question_seq = question_tokenizer.texts_to_sequences(train_df['question'])\n",
        "\n",
        "val_sql_seq = sql_tokenizer.texts_to_sequences(val_df['sql_query'])\n",
        "val_question_seq = question_tokenizer.texts_to_sequences(val_df['question'])\n",
        "\n",
        "test_sql_seq = sql_tokenizer.texts_to_sequences(test_df['sql_query'])\n",
        "test_question_seq = question_tokenizer.texts_to_sequences(test_df['question'])\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "max_sql_len = max(len(seq) for seq in train_sql_seq)\n",
        "max_question_len = max(len(seq) for seq in train_question_seq)\n",
        "\n",
        "train_sql_seq = pad_sequences(train_sql_seq, maxlen=max_sql_len, padding='post')\n",
        "train_question_seq = pad_sequences(train_question_seq, maxlen=max_question_len, padding='post')\n",
        "\n",
        "val_sql_seq = pad_sequences(val_sql_seq, maxlen=max_sql_len, padding='post')\n",
        "val_question_seq = pad_sequences(val_question_seq, maxlen=max_question_len, padding='post')\n",
        "\n",
        "test_sql_seq = pad_sequences(test_sql_seq, maxlen=max_sql_len, padding='post')\n",
        "test_question_seq = pad_sequences(test_question_seq, maxlen=max_question_len, padding='post')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the Model**"
      ],
      "metadata": {
        "id": "vIoknCpJG9sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "\n",
        "# Define the model\n",
        "embedding_dim = 256\n",
        "latent_dim = 512\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_sql_len,))\n",
        "enc_emb = Embedding(input_dim=len(sql_tokenizer.word_index) + 1, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_question_len,))\n",
        "dec_emb_layer = Embedding(input_dim=len(question_tokenizer.word_index) + 1, output_dim=embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(question_tokenizer.word_index) + 1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# encoder_input_data & decoder_input_data into decoder_target_data\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "D_ZQL_FlG-gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model**"
      ],
      "metadata": {
        "id": "ygrdl0D4HGOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the SQL queries and questions\n",
        "sql_tokenizer = Tokenizer()\n",
        "sql_tokenizer.fit_on_texts(train_df['sql_query'])\n",
        "\n",
        "question_tokenizer = Tokenizer()\n",
        "question_tokenizer.fit_on_texts(train_df['question'])\n",
        "\n",
        "# Convert text to sequences\n",
        "train_sql_seq = sql_tokenizer.texts_to_sequences(train_df['sql_query'])\n",
        "train_question_seq = question_tokenizer.texts_to_sequences(train_df['question'])\n",
        "\n",
        "val_sql_seq = sql_tokenizer.texts_to_sequences(val_df['sql_query'])\n",
        "val_question_seq = question_tokenizer.texts_to_sequences(val_df['question'])\n",
        "\n",
        "test_sql_seq = sql_tokenizer.texts_to_sequences(test_df['sql_query'])\n",
        "test_question_seq = question_tokenizer.texts_to_sequences(test_df['question'])\n",
        "\n",
        "# Determine max sequence lengths\n",
        "max_sql_len = max(max(len(seq) for seq in train_sql_seq), max(len(seq) for seq in val_sql_seq), max(len(seq) for seq in test_sql_seq))\n",
        "max_question_len = max(max(len(seq) for seq in train_question_seq), max(len(seq) for seq in val_question_seq), max(len(seq) for seq in test_question_seq))\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "train_sql_seq = pad_sequences(train_sql_seq, maxlen=max_sql_len, padding='post')\n",
        "train_question_seq = pad_sequences(train_question_seq, maxlen=max_question_len, padding='post')\n",
        "\n",
        "val_sql_seq = pad_sequences(val_sql_seq, maxlen=max_sql_len, padding='post')\n",
        "val_question_seq = pad_sequences(val_question_seq, maxlen=max_question_len, padding='post')\n",
        "\n",
        "test_sql_seq = pad_sequences(test_sql_seq, maxlen=max_sql_len, padding='post')\n",
        "test_question_seq = pad_sequences(test_question_seq, maxlen=max_question_len, padding='post')\n",
        "\n",
        "# Prepare decoder input and target sequences\n",
        "train_decoder_input_data = train_question_seq[:, :-1]\n",
        "train_target_data = train_question_seq[:, 1:]\n",
        "\n",
        "val_decoder_input_data = val_question_seq[:, :-1]\n",
        "val_target_data = val_question_seq[:, 1:]\n",
        "\n",
        "test_decoder_input_data = test_question_seq[:, :-1]\n",
        "test_target_data = test_question_seq[:, 1:]\n",
        "\n",
        "# Ensure the target data has the correct shape for the model's output\n",
        "train_target_data = train_target_data[..., None]\n",
        "val_target_data = val_target_data[..., None]\n",
        "test_target_data = test_target_data[..., None]\n"
      ],
      "metadata": {
        "id": "mAdBdgCeHCK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "\n",
        "# Define the model\n",
        "embedding_dim = 256\n",
        "latent_dim = 512\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_sql_len,))\n",
        "enc_emb = Embedding(input_dim=len(sql_tokenizer.word_index) + 1, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_question_len - 1,))\n",
        "dec_emb_layer = Embedding(input_dim=len(question_tokenizer.word_index) + 1, output_dim=embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(question_tokenizer.word_index) + 1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# encoder_input_data & decoder_input_data into decoder_target_data\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [train_sql_seq, train_decoder_input_data],\n",
        "    train_target_data,\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    validation_data=([val_sql_seq, val_decoder_input_data], val_target_data)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0AgMxbWHKpj",
        "outputId": "74136561-25e9-474f-bd67-3e8a91dcf3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 35)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)        [(None, 41)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, 35, 256)              1127680   ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)     (None, 41, 256)              1227520   ['input_4[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)               [(None, 512),                1574912   ['embedding_2[0][0]']         \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               [(None, 41, 512),            1574912   ['embedding_3[0][0]',         \n",
            "                              (None, 512),                           'lstm_2[0][1]',              \n",
            "                              (None, 512)]                           'lstm_2[0][2]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 41, 4795)             2459835   ['lstm_3[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7964859 (30.38 MB)\n",
            "Trainable params: 7964859 (30.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "47/47 [==============================] - 24s 439ms/step - loss: 2.5757 - accuracy: 0.7263 - val_loss: 1.6161 - val_accuracy: 0.7610\n",
            "Epoch 2/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.8088 - accuracy: 0.7535 - val_loss: 1.6930 - val_accuracy: 0.7739\n",
            "Epoch 3/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.7388 - accuracy: 0.7545 - val_loss: 1.4811 - val_accuracy: 0.7764\n",
            "Epoch 4/50\n",
            "47/47 [==============================] - 20s 426ms/step - loss: 1.6380 - accuracy: 0.7563 - val_loss: 1.4174 - val_accuracy: 0.7771\n",
            "Epoch 5/50\n",
            "47/47 [==============================] - 20s 425ms/step - loss: 1.6134 - accuracy: 0.7573 - val_loss: 1.4545 - val_accuracy: 0.7755\n",
            "Epoch 6/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.5962 - accuracy: 0.7594 - val_loss: 1.3881 - val_accuracy: 0.7866\n",
            "Epoch 7/50\n",
            "47/47 [==============================] - 20s 429ms/step - loss: 1.5679 - accuracy: 0.7651 - val_loss: 1.4255 - val_accuracy: 0.7650\n",
            "Epoch 8/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.5377 - accuracy: 0.7696 - val_loss: 1.3583 - val_accuracy: 0.7821\n",
            "Epoch 9/50\n",
            "47/47 [==============================] - 20s 426ms/step - loss: 1.5136 - accuracy: 0.7725 - val_loss: 1.3411 - val_accuracy: 0.7881\n",
            "Epoch 10/50\n",
            "47/47 [==============================] - 20s 426ms/step - loss: 1.4939 - accuracy: 0.7758 - val_loss: 1.3703 - val_accuracy: 0.7768\n",
            "Epoch 11/50\n",
            "47/47 [==============================] - 20s 429ms/step - loss: 1.4791 - accuracy: 0.7771 - val_loss: 1.3106 - val_accuracy: 0.7946\n",
            "Epoch 12/50\n",
            "47/47 [==============================] - 20s 432ms/step - loss: 1.4644 - accuracy: 0.7800 - val_loss: 1.3412 - val_accuracy: 0.7914\n",
            "Epoch 13/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.4510 - accuracy: 0.7818 - val_loss: 1.2949 - val_accuracy: 0.7977\n",
            "Epoch 14/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.4387 - accuracy: 0.7833 - val_loss: 1.3096 - val_accuracy: 0.7895\n",
            "Epoch 15/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.4289 - accuracy: 0.7838 - val_loss: 1.3582 - val_accuracy: 0.7853\n",
            "Epoch 16/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.4180 - accuracy: 0.7845 - val_loss: 1.2908 - val_accuracy: 0.7987\n",
            "Epoch 17/50\n",
            "47/47 [==============================] - 20s 430ms/step - loss: 1.4078 - accuracy: 0.7853 - val_loss: 1.2990 - val_accuracy: 0.7965\n",
            "Epoch 18/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.3983 - accuracy: 0.7860 - val_loss: 1.2947 - val_accuracy: 0.7961\n",
            "Epoch 19/50\n",
            "47/47 [==============================] - 20s 426ms/step - loss: 1.3891 - accuracy: 0.7869 - val_loss: 1.2954 - val_accuracy: 0.7886\n",
            "Epoch 20/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.3798 - accuracy: 0.7874 - val_loss: 1.2996 - val_accuracy: 0.7941\n",
            "Epoch 21/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.3705 - accuracy: 0.7882 - val_loss: 1.2697 - val_accuracy: 0.8021\n",
            "Epoch 22/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.3600 - accuracy: 0.7893 - val_loss: 1.3036 - val_accuracy: 0.7938\n",
            "Epoch 23/50\n",
            "47/47 [==============================] - 20s 429ms/step - loss: 1.3512 - accuracy: 0.7903 - val_loss: 1.2843 - val_accuracy: 0.7937\n",
            "Epoch 24/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.3410 - accuracy: 0.7908 - val_loss: 1.2801 - val_accuracy: 0.8019\n",
            "Epoch 25/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.3315 - accuracy: 0.7915 - val_loss: 1.2670 - val_accuracy: 0.8031\n",
            "Epoch 26/50\n",
            "47/47 [==============================] - 20s 429ms/step - loss: 1.3216 - accuracy: 0.7920 - val_loss: 1.2595 - val_accuracy: 0.8042\n",
            "Epoch 27/50\n",
            "47/47 [==============================] - 20s 436ms/step - loss: 1.3125 - accuracy: 0.7928 - val_loss: 1.2769 - val_accuracy: 0.7988\n",
            "Epoch 28/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.3033 - accuracy: 0.7933 - val_loss: 1.2446 - val_accuracy: 0.8038\n",
            "Epoch 29/50\n",
            "47/47 [==============================] - 20s 426ms/step - loss: 1.2936 - accuracy: 0.7941 - val_loss: 1.2434 - val_accuracy: 0.8035\n",
            "Epoch 30/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.2847 - accuracy: 0.7948 - val_loss: 1.2550 - val_accuracy: 0.8070\n",
            "Epoch 31/50\n",
            "47/47 [==============================] - 20s 430ms/step - loss: 1.2767 - accuracy: 0.7956 - val_loss: 1.2626 - val_accuracy: 0.7990\n",
            "Epoch 32/50\n",
            "47/47 [==============================] - 20s 430ms/step - loss: 1.2701 - accuracy: 0.7959 - val_loss: 1.2731 - val_accuracy: 0.8038\n",
            "Epoch 33/50\n",
            "47/47 [==============================] - 20s 426ms/step - loss: 1.2605 - accuracy: 0.7959 - val_loss: 1.2485 - val_accuracy: 0.8005\n",
            "Epoch 34/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.2520 - accuracy: 0.7970 - val_loss: 1.2441 - val_accuracy: 0.8086\n",
            "Epoch 35/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.2429 - accuracy: 0.7973 - val_loss: 1.2325 - val_accuracy: 0.8078\n",
            "Epoch 36/50\n",
            "47/47 [==============================] - 20s 430ms/step - loss: 1.2355 - accuracy: 0.7978 - val_loss: 1.2398 - val_accuracy: 0.8100\n",
            "Epoch 37/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.2272 - accuracy: 0.7989 - val_loss: 1.2156 - val_accuracy: 0.8069\n",
            "Epoch 38/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.2193 - accuracy: 0.7991 - val_loss: 1.2146 - val_accuracy: 0.8127\n",
            "Epoch 39/50\n",
            "47/47 [==============================] - 20s 426ms/step - loss: 1.2108 - accuracy: 0.7999 - val_loss: 1.2233 - val_accuracy: 0.8116\n",
            "Epoch 40/50\n",
            "47/47 [==============================] - 20s 428ms/step - loss: 1.2031 - accuracy: 0.8003 - val_loss: 1.2105 - val_accuracy: 0.8121\n",
            "Epoch 41/50\n",
            "47/47 [==============================] - 20s 430ms/step - loss: 1.1950 - accuracy: 0.8007 - val_loss: 1.2182 - val_accuracy: 0.8060\n",
            "Epoch 42/50\n",
            "47/47 [==============================] - 20s 434ms/step - loss: 1.1871 - accuracy: 0.8009 - val_loss: 1.2163 - val_accuracy: 0.8089\n",
            "Epoch 43/50\n",
            "47/47 [==============================] - 20s 429ms/step - loss: 1.1804 - accuracy: 0.8013 - val_loss: 1.2180 - val_accuracy: 0.8127\n",
            "Epoch 44/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.1709 - accuracy: 0.8019 - val_loss: 1.2237 - val_accuracy: 0.8117\n",
            "Epoch 45/50\n",
            "47/47 [==============================] - 20s 426ms/step - loss: 1.1639 - accuracy: 0.8022 - val_loss: 1.2146 - val_accuracy: 0.8147\n",
            "Epoch 46/50\n",
            "47/47 [==============================] - 20s 431ms/step - loss: 1.1567 - accuracy: 0.8028 - val_loss: 1.2198 - val_accuracy: 0.8125\n",
            "Epoch 47/50\n",
            "47/47 [==============================] - 20s 430ms/step - loss: 1.1484 - accuracy: 0.8033 - val_loss: 1.1990 - val_accuracy: 0.8160\n",
            "Epoch 48/50\n",
            "47/47 [==============================] - 20s 427ms/step - loss: 1.1407 - accuracy: 0.8039 - val_loss: 1.1950 - val_accuracy: 0.8177\n",
            "Epoch 49/50\n",
            "47/47 [==============================] - 20s 426ms/step - loss: 1.1336 - accuracy: 0.8040 - val_loss: 1.2147 - val_accuracy: 0.8116\n",
            "Epoch 50/50\n",
            "47/47 [==============================] - 20s 429ms/step - loss: 1.1261 - accuracy: 0.8045 - val_loss: 1.2155 - val_accuracy: 0.8130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "final validation loss < then the final train loss value ==>Overfit\n",
        "\n",
        "the last final train loss< than last final validation loss\n",
        "==>Overfit"
      ],
      "metadata": {
        "id": "4y5Ol5VaJFzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Retrieve the trained model from history\n",
        "model = history.model\n",
        "\n",
        "# Save the trained model\n",
        "model.save('model.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "VhobYeJZLveP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the tokenizer objects\n",
        "with open('sql_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(sql_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('question_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(question_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "lpu1nxF9NHZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Define the encoder model\n",
        "    encoder_model = Model(inputs=model.input[0], outputs=model.get_layer('lstm_2').output[1:])\n",
        "\n",
        "    # Define the decoder model\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs, state_h, state_c = model.get_layer('lstm_3')(\n",
        "        model.get_layer('embedding_3')(model.input[1]),\n",
        "        initial_state=decoder_states_inputs\n",
        "    )\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = model.get_layer('dense_1')(decoder_outputs)\n",
        "    decoder_model = Model(\n",
        "        [model.input[1]] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states\n",
        "    )\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Populate the first token of target sequence with the start token.\n",
        "    start_token_index = question_tokenizer.word_index['starttoken']\n",
        "    target_seq[0, 0] = start_token_index\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token within the vocabulary size\n",
        "        sampled_token_index = np.random.choice(len(output_tokens[0, -1, :]), p=output_tokens[0, -1, :])\n",
        "        sampled_word = question_tokenizer.index_word.get(sampled_token_index, '')  # Get the word corresponding to the index\n",
        "\n",
        "        # Exit condition: either hit max length or find stop token.\n",
        "        if (sampled_word == 'endtoken' or len(decoded_sentence.split()) >= max_question_len):\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Update the target sequence (length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ],
      "metadata": {
        "id": "TL2GncplONBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# Load the tokenizer objects\n",
        "with open('sql_tokenizer.pickle', 'rb') as handle:\n",
        "    sql_tokenizer = pickle.load(handle)\n",
        "\n",
        "\n",
        "# Load the tokenizer objects\n",
        "with open('question_tokenizer.pickle', 'rb') as handle:\n",
        "    question_tokenizer = pickle.load(handle)\n",
        "\n",
        "# Manually add 'starttoken' to the word index\n",
        "question_tokenizer.word_index['starttoken'] = len(question_tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "# Example SQL query\n",
        "input_sql_query = \"SELECT Name, Composer, UnitPrice FROM tracks LIMIT 10\"\n",
        "\n",
        "# Tokenize and pad the input SQL query\n",
        "input_sql_seq = sql_tokenizer.texts_to_sequences([input_sql_query])\n",
        "input_sql_seq = pad_sequences(input_sql_seq, maxlen=max_sql_len, padding='post')\n",
        "\n",
        "# Generate natural language question\n",
        "decoded_question = decode_sequence(input_sql_seq)\n",
        "print(\"Generated Question:\", decoded_question)\n"
      ],
      "metadata": {
        "id": "Xb48YctyNh2G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qEzlcNMlOrhc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}